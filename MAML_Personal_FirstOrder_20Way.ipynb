{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2251edc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as keras_backend\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import preprocessing\n",
    "from os import listdir\n",
    "from os.path import isdir, join\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproduction\n",
    "seed = 333\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b472cb76",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe70dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "project_path = r\"C:\\Users\\ktub2\\Dropbox\\family\\Kausthubh\\UW Madison\\Coursework\\ECE 539\\Project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdf2d2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for validation.\n"
     ]
    }
   ],
   "source": [
    "omni_path = join(project_path, r\"omniglot-processed-train\")\n",
    "omni_train_datasets = dict()\n",
    "omni_val_datasets = dict()\n",
    "\n",
    "for name in listdir(omni_path):\n",
    "    path = join(omni_path, name)\n",
    "    if isdir(path):\n",
    "        omni_train_datasets[name] = preprocessing.image_dataset_from_directory(path, label_mode='categorical',\n",
    "                                                             color_mode='grayscale', batch_size=1000, image_size=(28,28),\n",
    "                                                             seed=seed, validation_split=0.25, subset=\"training\")\n",
    "        omni_val_datasets[name] = preprocessing.image_dataset_from_directory(path, label_mode='categorical',\n",
    "                                                             color_mode='grayscale', batch_size=1000, image_size=(28,28),\n",
    "                                                             seed=seed, validation_split=0.25, subset=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6bb8034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 28, 28, 1) (100, 20)\n"
     ]
    }
   ],
   "source": [
    "def dataset_to_tensors(dataset):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for x, y in dataset:\n",
    "        xs.extend(x)\n",
    "        ys.extend(y)\n",
    "    # xs = np.array(xs)\n",
    "    # ys = np.array(ys)\n",
    "    return tf.convert_to_tensor(xs), tf.convert_to_tensor(ys)\n",
    "\n",
    "xs, ys = dataset_to_tensors(omni_val_datasets['Grantha'])\n",
    "print(xs.shape, ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c78cf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n",
      "Train dataset converted\n",
      "Val dataset converted\n"
     ]
    }
   ],
   "source": [
    "omni_train_data = []\n",
    "omni_train_labels = []\n",
    "omni_val_data = []\n",
    "omni_val_labels = []\n",
    "for i in omni_train_datasets.keys():\n",
    "    xs,ys = dataset_to_tensors(omni_train_datasets[i])\n",
    "    omni_train_data.append(xs)\n",
    "    omni_train_labels.append(ys)\n",
    "    print('Train dataset converted')\n",
    "    xs,ys = dataset_to_tensors(omni_val_datasets[i])\n",
    "    omni_val_data.append(xs)\n",
    "    omni_val_labels.append(ys)\n",
    "    print('Val dataset converted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58f350b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_datasets = []\n",
    "for i in range(len(omni_train_data)):\n",
    "    if(len(omni_val_data[i]) == 100):\n",
    "        complete_datasets.append( (omni_train_data[i],omni_train_labels[i],omni_val_data[i],omni_val_labels[i]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b94d25c",
   "metadata": {},
   "source": [
    "### Model definition and relevant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52d54fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    relu_initializer = tf.keras.initializers.HeNormal()\n",
    "    softmax_initializer = tf.keras.initializers.GlorotNormal()\n",
    "    \n",
    "    inputs = keras.Input(shape=(28,28,1))\n",
    "    for i in range(4):\n",
    "        if(i == 0):\n",
    "            x = layers.Conv2D(64, (3,3), kernel_initializer=relu_initializer, bias_initializer='zeros', \n",
    "                              activation='relu', padding='same')(inputs)\n",
    "        else:\n",
    "            x = layers.Conv2D(64, (3,3), kernel_initializer=relu_initializer, bias_initializer='zeros', \n",
    "                              activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPool2D()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(20, kernel_initializer=softmax_initializer)(x)\n",
    "    outputs = layers.Softmax()(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='maml_model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3439aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, x, y, loss_fn=keras.losses.CategoricalCrossentropy()):\n",
    "    logits = model(x)\n",
    "    loss = loss_fn(y, logits)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b78b0d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"maml_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 3, 3, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 20)                1300      \n",
      "_________________________________________________________________\n",
      "softmax_8 (Softmax)          (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 113,748\n",
      "Trainable params: 113,236\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "meow = create_model()\n",
    "meow.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcf2e87",
   "metadata": {},
   "source": [
    "### Meta Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "791266b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............\n",
      "Gradient check: -0.03458849\n",
      "Meta Update: Epoch Number 0\n",
      "Avg. Loss: tf.Tensor(11.768156, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.096850455\n",
      "Meta Update: Epoch Number 1\n",
      "Avg. Loss: tf.Tensor(10.148666, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.16864255\n",
      "Meta Update: Epoch Number 2\n",
      "Avg. Loss: tf.Tensor(9.214113, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.052750584\n",
      "Meta Update: Epoch Number 3\n",
      "Avg. Loss: tf.Tensor(7.057494, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.3940752\n",
      "Meta Update: Epoch Number 4\n",
      "Avg. Loss: tf.Tensor(6.115264, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.22535303\n",
      "Meta Update: Epoch Number 5\n",
      "Avg. Loss: tf.Tensor(5.052243, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.044938713\n",
      "Meta Update: Epoch Number 6\n",
      "Avg. Loss: tf.Tensor(4.520511, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.015853062\n",
      "Meta Update: Epoch Number 7\n",
      "Avg. Loss: tf.Tensor(3.9052188, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0815621\n",
      "Meta Update: Epoch Number 8\n",
      "Avg. Loss: tf.Tensor(3.4729593, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.12442131\n",
      "Meta Update: Epoch Number 9\n",
      "Avg. Loss: tf.Tensor(3.4950283, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.00010106433\n",
      "Meta Update: Epoch Number 10\n",
      "Avg. Loss: tf.Tensor(3.173664, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.006251353\n",
      "Meta Update: Epoch Number 11\n",
      "Avg. Loss: tf.Tensor(2.9662445, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.012693817\n",
      "Meta Update: Epoch Number 12\n",
      "Avg. Loss: tf.Tensor(3.04647, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.07474461\n",
      "Meta Update: Epoch Number 13\n",
      "Avg. Loss: tf.Tensor(2.9141862, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.037811715\n",
      "Meta Update: Epoch Number 14\n",
      "Avg. Loss: tf.Tensor(3.0118408, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.04607119\n",
      "Meta Update: Epoch Number 15\n",
      "Avg. Loss: tf.Tensor(2.8396447, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.049668834\n",
      "Meta Update: Epoch Number 16\n",
      "Avg. Loss: tf.Tensor(2.915724, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.06369014\n",
      "Meta Update: Epoch Number 17\n",
      "Avg. Loss: tf.Tensor(2.7988458, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.054985605\n",
      "Meta Update: Epoch Number 18\n",
      "Avg. Loss: tf.Tensor(2.733101, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0071785883\n",
      "Meta Update: Epoch Number 19\n",
      "Avg. Loss: tf.Tensor(2.703712, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 1.4233403e-05\n",
      "Meta Update: Epoch Number 20\n",
      "Avg. Loss: tf.Tensor(2.9009926, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.12699874\n",
      "Meta Update: Epoch Number 21\n",
      "Avg. Loss: tf.Tensor(2.73902, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.05080206\n",
      "Meta Update: Epoch Number 22\n",
      "Avg. Loss: tf.Tensor(2.6431491, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0338173\n",
      "Meta Update: Epoch Number 23\n",
      "Avg. Loss: tf.Tensor(2.5922616, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.100396335\n",
      "Meta Update: Epoch Number 24\n",
      "Avg. Loss: tf.Tensor(2.67917, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.037050754\n",
      "Meta Update: Epoch Number 25\n",
      "Avg. Loss: tf.Tensor(2.6502438, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.004030658\n",
      "Meta Update: Epoch Number 26\n",
      "Avg. Loss: tf.Tensor(2.7553744, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.010455649\n",
      "Meta Update: Epoch Number 27\n",
      "Avg. Loss: tf.Tensor(2.7267835, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.062448457\n",
      "Meta Update: Epoch Number 28\n",
      "Avg. Loss: tf.Tensor(2.608049, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.08556789\n",
      "Meta Update: Epoch Number 29\n",
      "Avg. Loss: tf.Tensor(2.8840725, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.021965254\n",
      "Meta Update: Epoch Number 30\n",
      "Avg. Loss: tf.Tensor(2.7709217, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.069344044\n",
      "Meta Update: Epoch Number 31\n",
      "Avg. Loss: tf.Tensor(2.7089112, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0041482076\n",
      "Meta Update: Epoch Number 32\n",
      "Avg. Loss: tf.Tensor(2.7996495, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.017691558\n",
      "Meta Update: Epoch Number 33\n",
      "Avg. Loss: tf.Tensor(2.6838958, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0044441028\n",
      "Meta Update: Epoch Number 34\n",
      "Avg. Loss: tf.Tensor(2.5550816, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0009154617\n",
      "Meta Update: Epoch Number 35\n",
      "Avg. Loss: tf.Tensor(2.6221917, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.01781972\n",
      "Meta Update: Epoch Number 36\n",
      "Avg. Loss: tf.Tensor(2.5377653, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.009913478\n",
      "Meta Update: Epoch Number 37\n",
      "Avg. Loss: tf.Tensor(2.5705032, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.028068941\n",
      "Meta Update: Epoch Number 38\n",
      "Avg. Loss: tf.Tensor(2.6207314, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.0017755963\n",
      "Meta Update: Epoch Number 39\n",
      "Avg. Loss: tf.Tensor(3.4236958, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.06738687\n",
      "Meta Update: Epoch Number 40\n",
      "Avg. Loss: tf.Tensor(2.4724565, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.011088218\n",
      "Meta Update: Epoch Number 41\n",
      "Avg. Loss: tf.Tensor(2.4572396, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.003815352\n",
      "Meta Update: Epoch Number 42\n",
      "Avg. Loss: tf.Tensor(2.3724372, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.035051726\n",
      "Meta Update: Epoch Number 43\n",
      "Avg. Loss: tf.Tensor(2.542953, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.017648742\n",
      "Meta Update: Epoch Number 44\n",
      "Avg. Loss: tf.Tensor(2.4874823, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.042418983\n",
      "Meta Update: Epoch Number 45\n",
      "Avg. Loss: tf.Tensor(2.4257205, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.014443959\n",
      "Meta Update: Epoch Number 46\n",
      "Avg. Loss: tf.Tensor(2.43539, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.025793726\n",
      "Meta Update: Epoch Number 47\n",
      "Avg. Loss: tf.Tensor(2.4241176, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.01702524\n",
      "Meta Update: Epoch Number 48\n",
      "Avg. Loss: tf.Tensor(2.34422, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.032404058\n",
      "Meta Update: Epoch Number 49\n",
      "Avg. Loss: tf.Tensor(2.438293, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.011505264\n",
      "Meta Update: Epoch Number 50\n",
      "Avg. Loss: tf.Tensor(2.2829356, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.044552267\n",
      "Meta Update: Epoch Number 51\n",
      "Avg. Loss: tf.Tensor(2.364641, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.06963574\n",
      "Meta Update: Epoch Number 52\n",
      "Avg. Loss: tf.Tensor(2.4963157, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.005569376\n",
      "Meta Update: Epoch Number 53\n",
      "Avg. Loss: tf.Tensor(2.2586513, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.05022274\n",
      "Meta Update: Epoch Number 54\n",
      "Avg. Loss: tf.Tensor(2.084387, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.01168552\n",
      "Meta Update: Epoch Number 55\n",
      "Avg. Loss: tf.Tensor(2.3198686, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.01670209\n",
      "Meta Update: Epoch Number 56\n",
      "Avg. Loss: tf.Tensor(2.4398105, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.075704694\n",
      "Meta Update: Epoch Number 57\n",
      "Avg. Loss: tf.Tensor(2.258448, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.041138463\n",
      "Meta Update: Epoch Number 58\n",
      "Avg. Loss: tf.Tensor(2.125188, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.020397656\n",
      "Meta Update: Epoch Number 59\n",
      "Avg. Loss: tf.Tensor(2.1497993, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.006172807\n",
      "Meta Update: Epoch Number 60\n",
      "Avg. Loss: tf.Tensor(2.2713664, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.005180314\n",
      "Meta Update: Epoch Number 61\n",
      "Avg. Loss: tf.Tensor(2.2976968, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.031875182\n",
      "Meta Update: Epoch Number 62\n",
      "Avg. Loss: tf.Tensor(2.2931457, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.040119186\n",
      "Meta Update: Epoch Number 63\n",
      "Avg. Loss: tf.Tensor(1.9289896, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............\n",
      "Gradient check: 0.15215169\n",
      "Meta Update: Epoch Number 64\n",
      "Avg. Loss: tf.Tensor(2.2533376, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.029443223\n",
      "Meta Update: Epoch Number 65\n",
      "Avg. Loss: tf.Tensor(2.005256, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.009597369\n",
      "Meta Update: Epoch Number 66\n",
      "Avg. Loss: tf.Tensor(2.0371535, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.026874531\n",
      "Meta Update: Epoch Number 67\n",
      "Avg. Loss: tf.Tensor(2.1632466, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.038551472\n",
      "Meta Update: Epoch Number 68\n",
      "Avg. Loss: tf.Tensor(2.0988662, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.04331421\n",
      "Meta Update: Epoch Number 69\n",
      "Avg. Loss: tf.Tensor(2.0558014, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.027935423\n",
      "Meta Update: Epoch Number 70\n",
      "Avg. Loss: tf.Tensor(2.2263865, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.03665677\n",
      "Meta Update: Epoch Number 71\n",
      "Avg. Loss: tf.Tensor(2.1056328, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.005256825\n",
      "Meta Update: Epoch Number 72\n",
      "Avg. Loss: tf.Tensor(1.9846716, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.031722482\n",
      "Meta Update: Epoch Number 73\n",
      "Avg. Loss: tf.Tensor(1.88134, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.032836054\n",
      "Meta Update: Epoch Number 74\n",
      "Avg. Loss: tf.Tensor(1.9736983, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.04452514\n",
      "Meta Update: Epoch Number 75\n",
      "Avg. Loss: tf.Tensor(1.9688777, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.008528961\n",
      "Meta Update: Epoch Number 76\n",
      "Avg. Loss: tf.Tensor(1.9840767, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.022740785\n",
      "Meta Update: Epoch Number 77\n",
      "Avg. Loss: tf.Tensor(2.0210323, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0060808165\n",
      "Meta Update: Epoch Number 78\n",
      "Avg. Loss: tf.Tensor(1.9316956, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.032315396\n",
      "Meta Update: Epoch Number 79\n",
      "Avg. Loss: tf.Tensor(1.8037924, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0035286518\n",
      "Meta Update: Epoch Number 80\n",
      "Avg. Loss: tf.Tensor(1.7628739, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.037794486\n",
      "Meta Update: Epoch Number 81\n",
      "Avg. Loss: tf.Tensor(1.8696673, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.011323468\n",
      "Meta Update: Epoch Number 82\n",
      "Avg. Loss: tf.Tensor(2.090304, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.03934204\n",
      "Meta Update: Epoch Number 83\n",
      "Avg. Loss: tf.Tensor(1.8338388, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0018139682\n",
      "Meta Update: Epoch Number 84\n",
      "Avg. Loss: tf.Tensor(1.6737853, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.0022294465\n",
      "Meta Update: Epoch Number 85\n",
      "Avg. Loss: tf.Tensor(1.7832301, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.028612081\n",
      "Meta Update: Epoch Number 86\n",
      "Avg. Loss: tf.Tensor(1.6367204, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.006338761\n",
      "Meta Update: Epoch Number 87\n",
      "Avg. Loss: tf.Tensor(1.9539776, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0057540084\n",
      "Meta Update: Epoch Number 88\n",
      "Avg. Loss: tf.Tensor(1.8299749, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.040454857\n",
      "Meta Update: Epoch Number 89\n",
      "Avg. Loss: tf.Tensor(1.7071598, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0068070237\n",
      "Meta Update: Epoch Number 90\n",
      "Avg. Loss: tf.Tensor(1.6521292, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.038058326\n",
      "Meta Update: Epoch Number 91\n",
      "Avg. Loss: tf.Tensor(1.6479766, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.008234413\n",
      "Meta Update: Epoch Number 92\n",
      "Avg. Loss: tf.Tensor(1.6040324, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.0018130466\n",
      "Meta Update: Epoch Number 93\n",
      "Avg. Loss: tf.Tensor(1.6327065, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0032511873\n",
      "Meta Update: Epoch Number 94\n",
      "Avg. Loss: tf.Tensor(1.8639538, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.044591915\n",
      "Meta Update: Epoch Number 95\n",
      "Avg. Loss: tf.Tensor(1.5948048, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.04692179\n",
      "Meta Update: Epoch Number 96\n",
      "Avg. Loss: tf.Tensor(1.6761907, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.0045520547\n",
      "Meta Update: Epoch Number 97\n",
      "Avg. Loss: tf.Tensor(1.6243932, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.0006861519\n",
      "Meta Update: Epoch Number 98\n",
      "Avg. Loss: tf.Tensor(1.6797628, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.0054205647\n",
      "Meta Update: Epoch Number 99\n",
      "Avg. Loss: tf.Tensor(1.618191, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.092477284\n",
      "Meta Update: Epoch Number 100\n",
      "Avg. Loss: tf.Tensor(1.8499215, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.018052593\n",
      "Meta Update: Epoch Number 101\n",
      "Avg. Loss: tf.Tensor(1.8593253, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.026844053\n",
      "Meta Update: Epoch Number 102\n",
      "Avg. Loss: tf.Tensor(1.447661, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.021529008\n",
      "Meta Update: Epoch Number 103\n",
      "Avg. Loss: tf.Tensor(1.348385, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.03488271\n",
      "Meta Update: Epoch Number 104\n",
      "Avg. Loss: tf.Tensor(1.4345741, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.016258953\n",
      "Meta Update: Epoch Number 105\n",
      "Avg. Loss: tf.Tensor(1.4436982, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.06259382\n",
      "Meta Update: Epoch Number 106\n",
      "Avg. Loss: tf.Tensor(1.4241002, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.016805166\n",
      "Meta Update: Epoch Number 107\n",
      "Avg. Loss: tf.Tensor(1.5235325, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.024284413\n",
      "Meta Update: Epoch Number 108\n",
      "Avg. Loss: tf.Tensor(1.4933093, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.022101447\n",
      "Meta Update: Epoch Number 109\n",
      "Avg. Loss: tf.Tensor(1.2992371, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.00054355897\n",
      "Meta Update: Epoch Number 110\n",
      "Avg. Loss: tf.Tensor(1.5541657, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.03398088\n",
      "Meta Update: Epoch Number 111\n",
      "Avg. Loss: tf.Tensor(1.4783245, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0089622075\n",
      "Meta Update: Epoch Number 112\n",
      "Avg. Loss: tf.Tensor(1.4909286, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.019918732\n",
      "Meta Update: Epoch Number 113\n",
      "Avg. Loss: tf.Tensor(1.4613057, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.028605966\n",
      "Meta Update: Epoch Number 114\n",
      "Avg. Loss: tf.Tensor(1.3412396, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.019494534\n",
      "Meta Update: Epoch Number 115\n",
      "Avg. Loss: tf.Tensor(1.2615927, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.01186383\n",
      "Meta Update: Epoch Number 116\n",
      "Avg. Loss: tf.Tensor(1.4755042, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.010542883\n",
      "Meta Update: Epoch Number 117\n",
      "Avg. Loss: tf.Tensor(1.5548928, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.000988137\n",
      "Meta Update: Epoch Number 118\n",
      "Avg. Loss: tf.Tensor(1.5368692, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.03842704\n",
      "Meta Update: Epoch Number 119\n",
      "Avg. Loss: tf.Tensor(1.254612, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.05342982\n",
      "Meta Update: Epoch Number 120\n",
      "Avg. Loss: tf.Tensor(1.5582368, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.042545903\n",
      "Meta Update: Epoch Number 121\n",
      "Avg. Loss: tf.Tensor(1.1273056, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.010856906\n",
      "Meta Update: Epoch Number 122\n",
      "Avg. Loss: tf.Tensor(1.2268192, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.021241657\n",
      "Meta Update: Epoch Number 123\n",
      "Avg. Loss: tf.Tensor(1.5023581, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.015108317\n",
      "Meta Update: Epoch Number 124\n",
      "Avg. Loss: tf.Tensor(1.3003856, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.009657387\n",
      "Meta Update: Epoch Number 125\n",
      "Avg. Loss: tf.Tensor(1.2522577, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.019808702\n",
      "Meta Update: Epoch Number 126\n",
      "Avg. Loss: tf.Tensor(1.4386337, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............\n",
      "Gradient check: 0.050290592\n",
      "Meta Update: Epoch Number 127\n",
      "Avg. Loss: tf.Tensor(1.1701012, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.026800644\n",
      "Meta Update: Epoch Number 128\n",
      "Avg. Loss: tf.Tensor(1.1653224, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0019288356\n",
      "Meta Update: Epoch Number 129\n",
      "Avg. Loss: tf.Tensor(1.2719198, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.029196722\n",
      "Meta Update: Epoch Number 130\n",
      "Avg. Loss: tf.Tensor(1.2339584, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.026173715\n",
      "Meta Update: Epoch Number 131\n",
      "Avg. Loss: tf.Tensor(1.5740852, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.012598605\n",
      "Meta Update: Epoch Number 132\n",
      "Avg. Loss: tf.Tensor(1.0285289, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.01708674\n",
      "Meta Update: Epoch Number 133\n",
      "Avg. Loss: tf.Tensor(1.1941475, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.033702236\n",
      "Meta Update: Epoch Number 134\n",
      "Avg. Loss: tf.Tensor(1.0321039, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.2464329\n",
      "Meta Update: Epoch Number 135\n",
      "Avg. Loss: tf.Tensor(1.3756281, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.040097095\n",
      "Meta Update: Epoch Number 136\n",
      "Avg. Loss: tf.Tensor(1.2371761, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.009114051\n",
      "Meta Update: Epoch Number 137\n",
      "Avg. Loss: tf.Tensor(1.1829473, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.021291811\n",
      "Meta Update: Epoch Number 138\n",
      "Avg. Loss: tf.Tensor(1.0877091, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0014443159\n",
      "Meta Update: Epoch Number 139\n",
      "Avg. Loss: tf.Tensor(1.2959329, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.038029388\n",
      "Meta Update: Epoch Number 140\n",
      "Avg. Loss: tf.Tensor(1.1105998, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.012302831\n",
      "Meta Update: Epoch Number 141\n",
      "Avg. Loss: tf.Tensor(1.1437627, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.015669622\n",
      "Meta Update: Epoch Number 142\n",
      "Avg. Loss: tf.Tensor(1.1406744, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.03575726\n",
      "Meta Update: Epoch Number 143\n",
      "Avg. Loss: tf.Tensor(0.93578434, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.01931146\n",
      "Meta Update: Epoch Number 144\n",
      "Avg. Loss: tf.Tensor(1.2222764, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.024036737\n",
      "Meta Update: Epoch Number 145\n",
      "Avg. Loss: tf.Tensor(1.2320753, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.015442064\n",
      "Meta Update: Epoch Number 146\n",
      "Avg. Loss: tf.Tensor(1.3276045, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.009594288\n",
      "Meta Update: Epoch Number 147\n",
      "Avg. Loss: tf.Tensor(0.9511806, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.0461505\n",
      "Meta Update: Epoch Number 148\n",
      "Avg. Loss: tf.Tensor(1.1020457, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.0018692445\n",
      "Meta Update: Epoch Number 149\n",
      "Avg. Loss: tf.Tensor(0.9818965, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.018166162\n",
      "Meta Update: Epoch Number 150\n",
      "Avg. Loss: tf.Tensor(1.0806022, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0017627785\n",
      "Meta Update: Epoch Number 151\n",
      "Avg. Loss: tf.Tensor(1.0163267, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.005958706\n",
      "Meta Update: Epoch Number 152\n",
      "Avg. Loss: tf.Tensor(1.2072022, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.0085647\n",
      "Meta Update: Epoch Number 153\n",
      "Avg. Loss: tf.Tensor(1.1304084, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.011297177\n",
      "Meta Update: Epoch Number 154\n",
      "Avg. Loss: tf.Tensor(0.9964026, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.008508964\n",
      "Meta Update: Epoch Number 155\n",
      "Avg. Loss: tf.Tensor(1.1758797, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.029434007\n",
      "Meta Update: Epoch Number 156\n",
      "Avg. Loss: tf.Tensor(0.9612586, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.008550578\n",
      "Meta Update: Epoch Number 157\n",
      "Avg. Loss: tf.Tensor(0.89617246, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.016842209\n",
      "Meta Update: Epoch Number 158\n",
      "Avg. Loss: tf.Tensor(1.0742687, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.004856227\n",
      "Meta Update: Epoch Number 159\n",
      "Avg. Loss: tf.Tensor(0.87020636, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.009737201\n",
      "Meta Update: Epoch Number 160\n",
      "Avg. Loss: tf.Tensor(1.1532036, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.08476583\n",
      "Meta Update: Epoch Number 161\n",
      "Avg. Loss: tf.Tensor(1.0946276, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.014919837\n",
      "Meta Update: Epoch Number 162\n",
      "Avg. Loss: tf.Tensor(1.0653641, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.004001513\n",
      "Meta Update: Epoch Number 163\n",
      "Avg. Loss: tf.Tensor(0.8854164, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.018380217\n",
      "Meta Update: Epoch Number 164\n",
      "Avg. Loss: tf.Tensor(1.0561858, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0015718732\n",
      "Meta Update: Epoch Number 165\n",
      "Avg. Loss: tf.Tensor(0.91775805, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.013461418\n",
      "Meta Update: Epoch Number 166\n",
      "Avg. Loss: tf.Tensor(0.94610083, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.008733056\n",
      "Meta Update: Epoch Number 167\n",
      "Avg. Loss: tf.Tensor(1.0497769, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.009714432\n",
      "Meta Update: Epoch Number 168\n",
      "Avg. Loss: tf.Tensor(0.8852522, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.009773753\n",
      "Meta Update: Epoch Number 169\n",
      "Avg. Loss: tf.Tensor(0.9225996, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0091437865\n",
      "Meta Update: Epoch Number 170\n",
      "Avg. Loss: tf.Tensor(0.9907735, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.027800303\n",
      "Meta Update: Epoch Number 171\n",
      "Avg. Loss: tf.Tensor(0.99219996, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.030821245\n",
      "Meta Update: Epoch Number 172\n",
      "Avg. Loss: tf.Tensor(1.0042915, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.0070386287\n",
      "Meta Update: Epoch Number 173\n",
      "Avg. Loss: tf.Tensor(0.98967093, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.017876752\n",
      "Meta Update: Epoch Number 174\n",
      "Avg. Loss: tf.Tensor(0.9961852, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.051791713\n",
      "Meta Update: Epoch Number 175\n",
      "Avg. Loss: tf.Tensor(0.83226454, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.020081338\n",
      "Meta Update: Epoch Number 176\n",
      "Avg. Loss: tf.Tensor(0.788425, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.006327988\n",
      "Meta Update: Epoch Number 177\n",
      "Avg. Loss: tf.Tensor(1.0337118, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.027991034\n",
      "Meta Update: Epoch Number 178\n",
      "Avg. Loss: tf.Tensor(0.8105432, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.02670987\n",
      "Meta Update: Epoch Number 179\n",
      "Avg. Loss: tf.Tensor(0.85866976, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.015660435\n",
      "Meta Update: Epoch Number 180\n",
      "Avg. Loss: tf.Tensor(0.86463976, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.028162625\n",
      "Meta Update: Epoch Number 181\n",
      "Avg. Loss: tf.Tensor(0.7739707, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0015426972\n",
      "Meta Update: Epoch Number 182\n",
      "Avg. Loss: tf.Tensor(0.73100936, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.016867243\n",
      "Meta Update: Epoch Number 183\n",
      "Avg. Loss: tf.Tensor(0.84641236, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.029587619\n",
      "Meta Update: Epoch Number 184\n",
      "Avg. Loss: tf.Tensor(0.84469354, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0031224615\n",
      "Meta Update: Epoch Number 185\n",
      "Avg. Loss: tf.Tensor(0.92571485, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0049570743\n",
      "Meta Update: Epoch Number 186\n",
      "Avg. Loss: tf.Tensor(0.74788016, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.002690075\n",
      "Meta Update: Epoch Number 187\n",
      "Avg. Loss: tf.Tensor(0.74892133, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0055099917\n",
      "Meta Update: Epoch Number 188\n",
      "Avg. Loss: tf.Tensor(0.8578638, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.16104484\n",
      "Meta Update: Epoch Number 189\n",
      "Avg. Loss: tf.Tensor(0.8746799, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............\n",
      "Gradient check: 0.03369949\n",
      "Meta Update: Epoch Number 190\n",
      "Avg. Loss: tf.Tensor(0.7741718, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.050437298\n",
      "Meta Update: Epoch Number 191\n",
      "Avg. Loss: tf.Tensor(0.8429472, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.023719495\n",
      "Meta Update: Epoch Number 192\n",
      "Avg. Loss: tf.Tensor(0.8188035, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.010344397\n",
      "Meta Update: Epoch Number 193\n",
      "Avg. Loss: tf.Tensor(0.867486, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.038854778\n",
      "Meta Update: Epoch Number 194\n",
      "Avg. Loss: tf.Tensor(0.7130238, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.008682287\n",
      "Meta Update: Epoch Number 195\n",
      "Avg. Loss: tf.Tensor(0.73883635, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0037194244\n",
      "Meta Update: Epoch Number 196\n",
      "Avg. Loss: tf.Tensor(0.8277493, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0052182814\n",
      "Meta Update: Epoch Number 197\n",
      "Avg. Loss: tf.Tensor(0.7523202, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.013635691\n",
      "Meta Update: Epoch Number 198\n",
      "Avg. Loss: tf.Tensor(0.6920341, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.0132297035\n",
      "Meta Update: Epoch Number 199\n",
      "Avg. Loss: tf.Tensor(0.61318815, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0048089144\n",
      "Meta Update: Epoch Number 200\n",
      "Avg. Loss: tf.Tensor(0.68995714, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.014090813\n",
      "Meta Update: Epoch Number 201\n",
      "Avg. Loss: tf.Tensor(0.5861441, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.0025493763\n",
      "Meta Update: Epoch Number 202\n",
      "Avg. Loss: tf.Tensor(0.76500744, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.0069467397\n",
      "Meta Update: Epoch Number 203\n",
      "Avg. Loss: tf.Tensor(0.6082821, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0117059285\n",
      "Meta Update: Epoch Number 204\n",
      "Avg. Loss: tf.Tensor(0.7266899, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.0012882323\n",
      "Meta Update: Epoch Number 205\n",
      "Avg. Loss: tf.Tensor(0.6698648, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.03476681\n",
      "Meta Update: Epoch Number 206\n",
      "Avg. Loss: tf.Tensor(0.8352901, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.020182189\n",
      "Meta Update: Epoch Number 207\n",
      "Avg. Loss: tf.Tensor(0.8005373, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.007312851\n",
      "Meta Update: Epoch Number 208\n",
      "Avg. Loss: tf.Tensor(0.6952275, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.028130699\n",
      "Meta Update: Epoch Number 209\n",
      "Avg. Loss: tf.Tensor(0.7547515, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.08487044\n",
      "Meta Update: Epoch Number 210\n",
      "Avg. Loss: tf.Tensor(0.8403757, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.022599757\n",
      "Meta Update: Epoch Number 211\n",
      "Avg. Loss: tf.Tensor(0.5369333, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0079233255\n",
      "Meta Update: Epoch Number 212\n",
      "Avg. Loss: tf.Tensor(0.50037223, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0020777201\n",
      "Meta Update: Epoch Number 213\n",
      "Avg. Loss: tf.Tensor(0.72145706, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.035608128\n",
      "Meta Update: Epoch Number 214\n",
      "Avg. Loss: tf.Tensor(0.60803574, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.05487574\n",
      "Meta Update: Epoch Number 215\n",
      "Avg. Loss: tf.Tensor(0.69631296, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.04048285\n",
      "Meta Update: Epoch Number 216\n",
      "Avg. Loss: tf.Tensor(0.6422769, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.0017631296\n",
      "Meta Update: Epoch Number 217\n",
      "Avg. Loss: tf.Tensor(0.56354153, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0043488573\n",
      "Meta Update: Epoch Number 218\n",
      "Avg. Loss: tf.Tensor(0.42892095, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.011369632\n",
      "Meta Update: Epoch Number 219\n",
      "Avg. Loss: tf.Tensor(0.5768413, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.00567583\n",
      "Meta Update: Epoch Number 220\n",
      "Avg. Loss: tf.Tensor(0.6234372, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.00093812635\n",
      "Meta Update: Epoch Number 221\n",
      "Avg. Loss: tf.Tensor(0.64349633, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.0066838963\n",
      "Meta Update: Epoch Number 222\n",
      "Avg. Loss: tf.Tensor(0.6400601, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.018622827\n",
      "Meta Update: Epoch Number 223\n",
      "Avg. Loss: tf.Tensor(0.65383923, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.062442135\n",
      "Meta Update: Epoch Number 224\n",
      "Avg. Loss: tf.Tensor(0.62346184, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0042687226\n",
      "Meta Update: Epoch Number 225\n",
      "Avg. Loss: tf.Tensor(0.599445, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.033662517\n",
      "Meta Update: Epoch Number 226\n",
      "Avg. Loss: tf.Tensor(0.62088823, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.015834155\n",
      "Meta Update: Epoch Number 227\n",
      "Avg. Loss: tf.Tensor(0.6317149, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: 0.0004571816\n",
      "Meta Update: Epoch Number 228\n",
      "Avg. Loss: tf.Tensor(0.6181366, shape=(), dtype=float32)\n",
      "...............\n",
      "Gradient check: -0.015834315\n",
      "Meta Update: Epoch Number 229\n",
      "Avg. Loss: tf.Tensor(0.75389045, shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-76f5a574256c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mmodel_copy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mmodel_copy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer_inner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCategoricalCrossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0minner_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_copy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minner_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_copy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     \u001b[0mforward_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs_with_tangents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_backward\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1930\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1931\u001b[1;33m       flat_outputs = forward_function.call(\n\u001b[0m\u001b[0;32m   1932\u001b[0m           \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs_with_tangents\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1933\u001b[0m           cancellation_manager=cancellation_manager)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "alpha = 0.1\n",
    "num_tasks = 15\n",
    "epochs = 1000\n",
    "inner_epochs = 3\n",
    "i = 0\n",
    "all_losses = []\n",
    "optimizer_inner = keras.optimizers.SGD(learning_rate=alpha)\n",
    "optimizer_outer = keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "model = create_model()\n",
    "model.compile()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    all_meta_gradients = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    for task in random.sample(complete_datasets,num_tasks):\n",
    "        train_data, train_labels, test_data, test_labels = task\n",
    "        \n",
    "        with tf.GradientTape() as test_tape:\n",
    "            model_copy = create_model()\n",
    "            model_copy.set_weights(model.get_weights())\n",
    "            model_copy.compile(optimizer=optimizer_inner, loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "            inner_history = model_copy.fit(train_data, train_labels, epochs=inner_epochs, verbose=0)\n",
    "            test_loss = compute_loss(model_copy, test_data, test_labels)\n",
    "            total_loss += test_loss\n",
    "            i += 1\n",
    "        print('.',end='')\n",
    "        meta_gradients = test_tape.gradient(test_loss, model_copy.trainable_weights)\n",
    "        all_meta_gradients.append(meta_gradients)\n",
    "      \n",
    "    print('')\n",
    "    print('Gradient check: '+str(all_meta_gradients[0][0][0,0,0,0].numpy()))\n",
    "    sum_meta_gradients = all_meta_gradients[0]\n",
    "    for i in range(1,len(all_meta_gradients)):\n",
    "        for j in range(len(all_meta_gradients[i])):\n",
    "            sum_meta_gradients[j] = sum_meta_gradients[j] + all_meta_gradients[i][j]\n",
    "    optimizer_outer.apply_gradients(zip(sum_meta_gradients, model.trainable_weights))\n",
    "    \n",
    "    print('Meta Update: Epoch Number '+str(epoch))\n",
    "    print('Avg. Loss: '+str(total_loss/(i)))\n",
    "    all_losses.append(total_loss/(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c94ff73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/full_maml_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save_weights('saved_models/weights_maml_model')\n",
    "model.save('saved_models/full_maml_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c9484f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss VS Epochs for 20-way classification')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_losses, 'orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Test Loss (Cross Entropy)')\n",
    "plt.title('Loss VS Epochs for 20-way classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d190ea6b",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70ada921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for validation.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 100 files for training.\n",
      "Found 400 files belonging to 20 classes.\n",
      "Using 300 files for validation.\n"
     ]
    }
   ],
   "source": [
    "omni_test_path = join(project_path, r\"omniglot-processed-test\")\n",
    "omni_test_train_datasets = dict()\n",
    "omni_test_val_datasets = dict()\n",
    "\n",
    "for name in listdir(omni_test_path):\n",
    "    path = join(omni_test_path, name)\n",
    "    if isdir(path):\n",
    "        omni_test_train_datasets[name] = preprocessing.image_dataset_from_directory(path, label_mode='categorical',\n",
    "                                                             color_mode='grayscale', batch_size=1000, image_size=(28,28),\n",
    "                                                             seed=seed, validation_split=0.75, subset=\"training\")\n",
    "        omni_test_val_datasets[name] = preprocessing.image_dataset_from_directory(path, label_mode='categorical',\n",
    "                                                             color_mode='grayscale', batch_size=1000, image_size=(28,28),\n",
    "                                                             seed=seed, validation_split=0.75, subset=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7de44ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test train dataset converted\n",
      "Test val dataset converted\n",
      "Test train dataset converted\n",
      "Test val dataset converted\n",
      "Test train dataset converted\n",
      "Test val dataset converted\n"
     ]
    }
   ],
   "source": [
    "omni_test_train_data = []\n",
    "omni_test_train_labels = []\n",
    "omni_test_val_data = []\n",
    "omni_test_val_labels = []\n",
    "\n",
    "for i in omni_test_train_datasets.keys():\n",
    "    xs,ys = dataset_to_tensors(omni_test_train_datasets[i])\n",
    "    omni_test_train_data.append(xs)\n",
    "    omni_test_train_labels.append(ys)\n",
    "    print('Test train dataset converted')\n",
    "    xs,ys = dataset_to_tensors(omni_test_val_datasets[i])\n",
    "    omni_test_val_data.append(xs)\n",
    "    omni_test_val_labels.append(ys)\n",
    "    print('Test val dataset converted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a9ef4a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_fine_tuning_data = omni_test_train_data[0]\n",
    "greek_fine_tuning_labels = omni_test_train_labels[0]\n",
    "\n",
    "greek_val_data = omni_test_val_data[0]\n",
    "greek_val_labels = omni_test_val_labels[0]\n",
    "\n",
    "# model_greek = create_model()\n",
    "# model_greek.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f1a1109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_greek = keras.models.load_model('saved_models/full_maml_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c96f9a81",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4/4 - 1s - loss: 5.1577 - accuracy: 0.0700\n",
      "Epoch 2/200\n",
      "4/4 - 1s - loss: 2.2454 - accuracy: 0.4800\n",
      "Epoch 3/200\n",
      "4/4 - 1s - loss: 1.2396 - accuracy: 0.7000\n",
      "Epoch 4/200\n",
      "4/4 - 1s - loss: 0.7010 - accuracy: 0.8800\n",
      "Epoch 5/200\n",
      "4/4 - 1s - loss: 0.4208 - accuracy: 0.9700\n",
      "Epoch 6/200\n",
      "4/4 - 1s - loss: 0.2428 - accuracy: 0.9900\n",
      "Epoch 7/200\n",
      "4/4 - 1s - loss: 0.1775 - accuracy: 1.0000\n",
      "Epoch 8/200\n",
      "4/4 - 1s - loss: 0.1337 - accuracy: 1.0000\n",
      "Epoch 9/200\n",
      "4/4 - 1s - loss: 0.1057 - accuracy: 0.9900\n",
      "Epoch 10/200\n",
      "4/4 - 1s - loss: 0.0689 - accuracy: 1.0000\n",
      "Epoch 11/200\n",
      "4/4 - 1s - loss: 0.0674 - accuracy: 1.0000\n",
      "Epoch 12/200\n",
      "4/4 - 1s - loss: 0.0631 - accuracy: 1.0000\n",
      "Epoch 13/200\n",
      "4/4 - 1s - loss: 0.0444 - accuracy: 1.0000\n",
      "Epoch 14/200\n",
      "4/4 - 1s - loss: 0.0460 - accuracy: 1.0000\n",
      "Epoch 15/200\n",
      "4/4 - 1s - loss: 0.0370 - accuracy: 1.0000\n",
      "Epoch 16/200\n",
      "4/4 - 1s - loss: 0.0335 - accuracy: 1.0000\n",
      "Epoch 17/200\n",
      "4/4 - 1s - loss: 0.0233 - accuracy: 1.0000\n",
      "Epoch 18/200\n",
      "4/4 - 1s - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 19/200\n",
      "4/4 - 1s - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 20/200\n",
      "4/4 - 1s - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 21/200\n",
      "4/4 - 1s - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 22/200\n",
      "4/4 - 1s - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 23/200\n",
      "4/4 - 1s - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 24/200\n",
      "4/4 - 1s - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 25/200\n",
      "4/4 - 1s - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "4/4 - 1s - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "4/4 - 1s - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "4/4 - 1s - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "4/4 - 1s - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "4/4 - 1s - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "4/4 - 1s - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "4/4 - 1s - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "4/4 - 1s - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "4/4 - 1s - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "4/4 - 1s - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "4/4 - 1s - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "4/4 - 1s - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "4/4 - 1s - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 39/200\n",
      "4/4 - 1s - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "4/4 - 1s - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "4/4 - 1s - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "4/4 - 1s - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "4/4 - 1s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "4/4 - 1s - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "4/4 - 1s - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "4/4 - 1s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "4/4 - 1s - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "4/4 - 1s - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "4/4 - 1s - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "4/4 - 1s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "4/4 - 1s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "4/4 - 1s - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "4/4 - 1s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "4/4 - 1s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "4/4 - 1s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "4/4 - 1s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "4/4 - 1s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "4/4 - 1s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "4/4 - 1s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "4/4 - 1s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "4/4 - 1s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "4/4 - 1s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "4/4 - 1s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "4/4 - 1s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "4/4 - 1s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "4/4 - 1s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "4/4 - 1s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "4/4 - 1s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "4/4 - 1s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "4/4 - 1s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "4/4 - 1s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "4/4 - 1s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "4/4 - 1s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "4/4 - 1s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "4/4 - 1s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "4/4 - 1s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "4/4 - 1s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "4/4 - 1s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "4/4 - 1s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "4/4 - 1s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "4/4 - 1s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "4/4 - 1s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "4/4 - 1s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "4/4 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "4/4 - 1s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "4/4 - 1s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "4/4 - 1s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "4/4 - 1s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "4/4 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "4/4 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "4/4 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "4/4 - 1s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "4/4 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "4/4 - 1s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "4/4 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "4/4 - 1s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "4/4 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "4/4 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "4/4 - 1s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "4/4 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "4/4 - 1s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "4/4 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "4/4 - 1s - loss: 9.8037e-04 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "4/4 - 1s - loss: 9.1924e-04 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "4/4 - 1s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "4/4 - 1s - loss: 9.7728e-04 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "4/4 - 1s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "4/4 - 1s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "4/4 - 1s - loss: 9.6482e-04 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "4/4 - 1s - loss: 8.2007e-04 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "4/4 - 1s - loss: 9.4427e-04 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "4/4 - 1s - loss: 9.0071e-04 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "4/4 - 1s - loss: 9.3553e-04 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "4/4 - 1s - loss: 9.2720e-04 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "4/4 - 1s - loss: 7.8421e-04 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "4/4 - 1s - loss: 7.5230e-04 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "4/4 - 1s - loss: 7.3330e-04 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "4/4 - 1s - loss: 6.9256e-04 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "4/4 - 1s - loss: 8.4748e-04 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "4/4 - 1s - loss: 6.8913e-04 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "4/4 - 1s - loss: 8.2335e-04 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "4/4 - 1s - loss: 7.4578e-04 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "4/4 - 1s - loss: 6.7544e-04 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "4/4 - 1s - loss: 6.6380e-04 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "4/4 - 1s - loss: 6.7487e-04 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "4/4 - 1s - loss: 6.2785e-04 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "4/4 - 1s - loss: 9.5543e-04 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "4/4 - 1s - loss: 6.1923e-04 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "4/4 - 1s - loss: 7.5903e-04 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "4/4 - 1s - loss: 7.4057e-04 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "4/4 - 1s - loss: 6.7384e-04 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "4/4 - 1s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "4/4 - 1s - loss: 8.4338e-04 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "4/4 - 1s - loss: 6.9259e-04 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "4/4 - 1s - loss: 6.0262e-04 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "4/4 - 1s - loss: 7.2057e-04 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "4/4 - 1s - loss: 6.2808e-04 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "4/4 - 1s - loss: 9.3601e-04 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "4/4 - 1s - loss: 7.1133e-04 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "4/4 - 1s - loss: 6.9078e-04 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "4/4 - 1s - loss: 6.2340e-04 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "4/4 - 1s - loss: 6.7427e-04 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "4/4 - 1s - loss: 6.0275e-04 - accuracy: 1.0000\n",
      "Epoch 144/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 - 1s - loss: 6.5829e-04 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "4/4 - 1s - loss: 7.5848e-04 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "4/4 - 1s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "4/4 - 1s - loss: 6.7341e-04 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "4/4 - 1s - loss: 7.5081e-04 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "4/4 - 1s - loss: 5.6889e-04 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "4/4 - 1s - loss: 7.1217e-04 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "4/4 - 1s - loss: 6.4164e-04 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "4/4 - 1s - loss: 5.4024e-04 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "4/4 - 1s - loss: 5.1198e-04 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "4/4 - 1s - loss: 5.4372e-04 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "4/4 - 1s - loss: 5.9746e-04 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "4/4 - 1s - loss: 5.3436e-04 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "4/4 - 1s - loss: 5.3881e-04 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "4/4 - 1s - loss: 6.0385e-04 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "4/4 - 1s - loss: 5.0873e-04 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "4/4 - 1s - loss: 4.7645e-04 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "4/4 - 1s - loss: 6.7399e-04 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "4/4 - 1s - loss: 8.5190e-04 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "4/4 - 1s - loss: 4.8338e-04 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "4/4 - 1s - loss: 7.0743e-04 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "4/4 - 1s - loss: 6.0812e-04 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "4/4 - 1s - loss: 4.8189e-04 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "4/4 - 1s - loss: 5.2440e-04 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "4/4 - 1s - loss: 5.4742e-04 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "4/4 - 1s - loss: 5.1034e-04 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "4/4 - 1s - loss: 5.5961e-04 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "4/4 - 1s - loss: 4.5856e-04 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "4/4 - 1s - loss: 4.4719e-04 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "4/4 - 1s - loss: 4.4665e-04 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "4/4 - 1s - loss: 4.9325e-04 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "4/4 - 1s - loss: 4.4563e-04 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "4/4 - 1s - loss: 4.1623e-04 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "4/4 - 1s - loss: 9.6351e-04 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "4/4 - 1s - loss: 4.9319e-04 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "4/4 - 1s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "4/4 - 1s - loss: 5.5491e-04 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "4/4 - 1s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "4/4 - 1s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "4/4 - 1s - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "4/4 - 1s - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "4/4 - 1s - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "4/4 - 1s - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "4/4 - 1s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "4/4 - 1s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "4/4 - 1s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "4/4 - 1s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "4/4 - 1s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "4/4 - 1s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "4/4 - 1s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "4/4 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "4/4 - 1s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "4/4 - 1s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "4/4 - 1s - loss: 9.4680e-04 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "4/4 - 1s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "4/4 - 1s - loss: 9.5513e-04 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "4/4 - 1s - loss: 0.0010 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x218842963d0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_greek.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                    loss=tf.keras.losses.CategoricalCrossentropy(), metrics='accuracy')\n",
    "model_greek.fit(greek_fine_tuning_data, greek_fine_tuning_labels, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "935ae0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 91ms/step - loss: 1.6678 - accuracy: 0.5267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6677799224853516, 0.5266666412353516]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_greek.evaluate(greek_val_data, greek_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "94bd62a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4/4 - 1s - loss: 4.1618 - accuracy: 0.0500\n",
      "Epoch 2/200\n",
      "4/4 - 1s - loss: 1.8089 - accuracy: 0.5600\n",
      "Epoch 3/200\n",
      "4/4 - 1s - loss: 1.1531 - accuracy: 0.7900\n",
      "Epoch 4/200\n",
      "4/4 - 1s - loss: 0.7663 - accuracy: 0.8900\n",
      "Epoch 5/200\n",
      "4/4 - 1s - loss: 0.5411 - accuracy: 0.9700\n",
      "Epoch 6/200\n",
      "4/4 - 1s - loss: 0.3651 - accuracy: 0.9900\n",
      "Epoch 7/200\n",
      "4/4 - 1s - loss: 0.2457 - accuracy: 0.9900\n",
      "Epoch 8/200\n",
      "4/4 - 1s - loss: 0.1418 - accuracy: 1.0000\n",
      "Epoch 9/200\n",
      "4/4 - 1s - loss: 0.0916 - accuracy: 1.0000\n",
      "Epoch 10/200\n",
      "4/4 - 1s - loss: 0.0718 - accuracy: 1.0000\n",
      "Epoch 11/200\n",
      "4/4 - 1s - loss: 0.0466 - accuracy: 1.0000\n",
      "Epoch 12/200\n",
      "4/4 - 1s - loss: 0.0349 - accuracy: 1.0000\n",
      "Epoch 13/200\n",
      "4/4 - 1s - loss: 0.0243 - accuracy: 1.0000\n",
      "Epoch 14/200\n",
      "4/4 - 1s - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 15/200\n",
      "4/4 - 1s - loss: 0.0168 - accuracy: 1.0000\n",
      "Epoch 16/200\n",
      "4/4 - 1s - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 17/200\n",
      "4/4 - 1s - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 18/200\n",
      "4/4 - 1s - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 19/200\n",
      "4/4 - 1s - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 20/200\n",
      "4/4 - 1s - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 21/200\n",
      "4/4 - 1s - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 22/200\n",
      "4/4 - 1s - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 23/200\n",
      "4/4 - 1s - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 24/200\n",
      "4/4 - 1s - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 25/200\n",
      "4/4 - 2s - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "4/4 - 1s - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "4/4 - 1s - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "4/4 - 2s - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "4/4 - 2s - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "4/4 - 2s - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "4/4 - 1s - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "4/4 - 1s - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "4/4 - 1s - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "4/4 - 1s - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "4/4 - 1s - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "4/4 - 2s - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "4/4 - 1s - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "4/4 - 1s - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 39/200\n",
      "4/4 - 1s - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "4/4 - 1s - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "4/4 - 2s - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "4/4 - 1s - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "4/4 - 1s - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "4/4 - 1s - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "4/4 - 1s - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "4/4 - 1s - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "4/4 - 1s - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "4/4 - 1s - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "4/4 - 1s - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "4/4 - 2s - loss: 0.0176 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "4/4 - 2s - loss: 0.0305 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "4/4 - 2s - loss: 0.0313 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "4/4 - 1s - loss: 0.0311 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "4/4 - 1s - loss: 0.0206 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "4/4 - 1s - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "4/4 - 1s - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "4/4 - 1s - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "4/4 - 1s - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "4/4 - 1s - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "4/4 - 1s - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "4/4 - 1s - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "4/4 - 1s - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "4/4 - 1s - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "4/4 - 1s - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "4/4 - 1s - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "4/4 - 1s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "4/4 - 1s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "4/4 - 1s - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "4/4 - 1s - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "4/4 - 1s - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "4/4 - 1s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "4/4 - 1s - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "4/4 - 1s - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "4/4 - 1s - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "4/4 - 1s - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "4/4 - 1s - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "4/4 - 1s - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "4/4 - 1s - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "4/4 - 1s - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "4/4 - 1s - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "4/4 - 1s - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "4/4 - 1s - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "4/4 - 1s - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "4/4 - 1s - loss: 0.0219 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "4/4 - 1s - loss: 0.0602 - accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "4/4 - 1s - loss: 0.0518 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "4/4 - 1s - loss: 0.0478 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "4/4 - 1s - loss: 0.0953 - accuracy: 0.9700\n",
      "Epoch 89/200\n",
      "4/4 - 1s - loss: 0.0433 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "4/4 - 1s - loss: 0.0346 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "4/4 - 1s - loss: 0.0499 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "4/4 - 1s - loss: 0.0526 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "4/4 - 1s - loss: 0.1002 - accuracy: 0.9700\n",
      "Epoch 94/200\n",
      "4/4 - 1s - loss: 0.0717 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "4/4 - 1s - loss: 0.1056 - accuracy: 0.9900\n",
      "Epoch 96/200\n",
      "4/4 - 1s - loss: 0.1341 - accuracy: 0.9700\n",
      "Epoch 97/200\n",
      "4/4 - 1s - loss: 0.1075 - accuracy: 0.9900\n",
      "Epoch 98/200\n",
      "4/4 - 1s - loss: 0.0645 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "4/4 - 1s - loss: 0.1058 - accuracy: 0.9900\n",
      "Epoch 100/200\n",
      "4/4 - 1s - loss: 0.0686 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "4/4 - 1s - loss: 0.0702 - accuracy: 0.9900\n",
      "Epoch 102/200\n",
      "4/4 - 1s - loss: 0.0606 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "4/4 - 1s - loss: 0.0299 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "4/4 - 1s - loss: 0.0259 - accuracy: 0.9900\n",
      "Epoch 105/200\n",
      "4/4 - 1s - loss: 0.0220 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "4/4 - 1s - loss: 0.0267 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "4/4 - 1s - loss: 0.0397 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "4/4 - 1s - loss: 0.0239 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "4/4 - 1s - loss: 0.0330 - accuracy: 0.9900\n",
      "Epoch 110/200\n",
      "4/4 - 1s - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "4/4 - 1s - loss: 0.0231 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "4/4 - 1s - loss: 0.0208 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "4/4 - 1s - loss: 0.0192 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "4/4 - 1s - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "4/4 - 1s - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "4/4 - 1s - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "4/4 - 1s - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "4/4 - 1s - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "4/4 - 1s - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "4/4 - 1s - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "4/4 - 1s - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "4/4 - 1s - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "4/4 - 1s - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "4/4 - 1s - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "4/4 - 1s - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "4/4 - 1s - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "4/4 - 1s - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "4/4 - 1s - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "4/4 - 1s - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "4/4 - 1s - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "4/4 - 1s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "4/4 - 1s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "4/4 - 1s - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "4/4 - 1s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "4/4 - 1s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "4/4 - 1s - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "4/4 - 1s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "4/4 - 1s - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "4/4 - 1s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "4/4 - 1s - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "4/4 - 1s - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "4/4 - 1s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "4/4 - 1s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "4/4 - 1s - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "4/4 - 1s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "4/4 - 1s - loss: 0.0033 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/200\n",
      "4/4 - 1s - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "4/4 - 1s - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "4/4 - 1s - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "4/4 - 1s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "4/4 - 1s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "4/4 - 1s - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "4/4 - 1s - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "4/4 - 1s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "4/4 - 1s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "4/4 - 1s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "4/4 - 1s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "4/4 - 1s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "4/4 - 1s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "4/4 - 1s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "4/4 - 1s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "4/4 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "4/4 - 1s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "4/4 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "4/4 - 1s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "4/4 - 1s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "4/4 - 1s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "4/4 - 1s - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "4/4 - 1s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "4/4 - 1s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "4/4 - 1s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "4/4 - 1s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "4/4 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "4/4 - 1s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "4/4 - 1s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "4/4 - 1s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "4/4 - 1s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "4/4 - 1s - loss: 0.0168 - accuracy: 0.9900\n",
      "Epoch 179/200\n",
      "4/4 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "4/4 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "4/4 - 1s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "4/4 - 1s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "4/4 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "4/4 - 1s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "4/4 - 1s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "4/4 - 1s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "4/4 - 1s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "4/4 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "4/4 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "4/4 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "4/4 - 1s - loss: 7.0491e-04 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "4/4 - 1s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "4/4 - 1s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "4/4 - 1s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "4/4 - 1s - loss: 9.8377e-04 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "4/4 - 1s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "4/4 - 1s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "4/4 - 1s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "4/4 - 1s - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "4/4 - 1s - loss: 0.0034 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21b5d8eab80>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_greek_meow = create_model()\n",
    "model_greek_meow.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                    loss=tf.keras.losses.CategoricalCrossentropy(), metrics='accuracy')\n",
    "model_greek_meow.fit(greek_fine_tuning_data, greek_fine_tuning_labels, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d941af68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 2s 151ms/step - loss: 2.8423 - accuracy: 0.3533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.842278242111206, 0.35333332419395447]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_greek_meow.evaluate(greek_val_data, greek_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f848ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
